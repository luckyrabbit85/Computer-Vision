{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ac91db",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "\n",
    "This is the paper in 2017 CVPR which got Best Paper Award with over 2000 citations. It is jointly invented by Cornwell University, Tsinghua University and Facebook AI Research (FAIR).\n",
    "With dense connection, fewer parameters and high accuracy are achieved compared with ResNet and Pre-Activation ResNet. So, let’s see how it works\n",
    "\n",
    "### Dense Block\n",
    "\n",
    "+ In Standard ConvNet, input image goes through multiple convolution and obtain high-level features.\n",
    "+ In ResNet, identity mapping is proposed to promote the gradient propagation. Element-wise addition is used. It can be viewed as algorithms with a state passed from one ResNet module to another one.\n",
    "+ In DenseNet, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Concatenation is used. Each layer is receiving a “collective knowledge” from all preceding layers.\n",
    "\n",
    "![](./fig/dense.png)\n",
    "\n",
    "Since each layer receives feature maps from all preceding layers, network can be thinner and compact, i.e. number of channels can be fewer. The growth rate k is the additional number of channels for each layer.\n",
    "So, it have higher computational efficiency and memory efficiency. The following figure shows the concept of concatenation during forward propagation:\n",
    "\n",
    "![](./fig/conc.gif)\n",
    "\n",
    "\n",
    "### DenseNet Architecture\n",
    "\n",
    "**Basic DenseNet Composition Layer**\n",
    "\n",
    "![](./fig/comp.gif)\n",
    "\n",
    "For each composition layer, Pre-Activation Batch Norm (BN) and ReLU, then 3×3 Conv are done with output feature maps of k channels, say for example, to transform x0, x1, x2, x3 to x4. This is the idea from Pre-Activation ResNet.\n",
    "\n",
    "**DenseNet-B (Bottleneck Layers)**\n",
    "\n",
    "![](./fig/b.png)\n",
    "\n",
    "To reduce the model complexity and size, BN-ReLU-1×1 Conv is done before BN-ReLU-3×3 Conv.\n",
    "\n",
    "**Multiple Dense Blocks with Transition Layers**\n",
    "\n",
    "![](./fig/m.png)\n",
    "\n",
    "1×1 Conv followed by 2×2 average pooling are used as the transition layers between two contiguous dense blocks.\n",
    "Feature map sizes are the same within the dense block so that they can be concatenated together easily.\n",
    "At the end of the last dense block, a global average pooling is performed and then a softmax classifier is attached.\n",
    "\n",
    "**DenseNet-BC (Further Compression)**\n",
    "\n",
    "If a dense block contains m feature-maps, The transition layer generate θm output feature maps, where 0<θ≤1 is referred to as the compression factor.\n",
    "When θ=1, the number of feature-maps across transition layers remains unchanged. DenseNet with θ<1 is referred as DenseNet-C, and θ=0.5 in the experiment. \n",
    "When both the bottleneck and transition layers with θ<1 are used, the model is referred as DenseNet-BC.\n",
    "Finally, DenseNets with/without B/C and with different L layers and k growth rate are trained.\n",
    "\n",
    "### Advantages of DenseNet\n",
    "\n",
    "+ Strong Gradient Flow  \n",
    "The error signal can be easily propagated to earlier layers more directly. This is a kind of implicit deep supervision as earlier layers can get direct supervision from the final classification layer.\n",
    "\n",
    "+ Parameter & Computational Efficiency  \n",
    "For each layer, number of parameters in ResNet is directly proportional to C×C while Number of parameters in DenseNet is directly proportional to l×k×k. Since k<<C, DenseNet has much smaller size than ResNet.\n",
    "\n",
    "+ More Diversified Features  \n",
    "Since each layer in DenseNet receive all preceding layers as input, more diversified features and tends to have richer patterns.\n",
    "\n",
    "+ Maintains Low Complexity Features  \n",
    "In *Standard ConvNet*, classifier uses most complex features. In *DenseNet*, classifier uses features of all complexity levels. It tends to give more smooth decision boundaries. It also explains why DenseNet performs well when training data is insufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361388c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
