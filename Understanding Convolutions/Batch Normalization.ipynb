{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3636d974",
   "metadata": {},
   "source": [
    "# Batch-Normalization\n",
    "\n",
    "Batch-Normalization (BN) is an algorithmic method which makes the training of Deep Neural Networks (DNN) faster and more stable.\n",
    "It consists of normalizing activation vectors from hidden layers using the first and the second statistical moments (mean and variance) of the current batch. This normalization step is applied right before (or right after) the nonlinear function.\n",
    "\n",
    "![](./fig/bn1.jpeg)\n",
    "\n",
    "![](./fig/bn2.jpeg)\n",
    "\n",
    "Batch normalization is computed differently during the training and the testing phase. At each hidden layer, Batch Normalization transforms the signal as follow :\n",
    "\n",
    "![](./fig/bn3.gif)\n",
    "![](./fig/bn4.gif)\n",
    "\n",
    "The BN layer first determines the mean 𝜇 and the standard deviation σ of the activation values across the batch, using (1) and (2).\n",
    "It then normalizes the activation vector Z^(i) with (3). That way, each neuron’s output follows a standard normal distribution across the batch. (𝜀 is a constant used for numerical stability)\n",
    "\n",
    "![](./fig/bn3.jpeg)\n",
    "\n",
    "It finally calculates the layer’s output Ẑ(i) by applying a linear transformation with 𝛾 and 𝛽, two trainable parameters (4). Such step allows the model to choose the optimum distribution for each hidden layers, by adjusting those two parameters :\n",
    "\n",
    "+ 𝛾 allows to adjust the standard deviation ;\n",
    "+ 𝛽 allows to adjust the bias, shifting the curve on the right or on the left side.\n",
    "\n",
    "At each iteration, the network computes the mean 𝜇 and the standard deviation σ corresponding to the current batch. Then it trains 𝛾 and 𝛽 through gradient descent, using an Exponential Moving Average (EMA) to give more importance to the latest iterations.\n",
    "\n",
    "![](./fig/3.jpeg)\n",
    "\n",
    "Unlike the training phase, we may not have a full batch to feed into the model during the evaluation phase. To tackle this issue, we compute (𝜇_pop , σ_pop), such as :  \n",
    "\n",
    "+ 𝜇_pop : estimated mean of the studied population ;\n",
    "+ σ_pop : estimated standard-deviation of the studied population.  \n",
    "\n",
    "Those values are computed using all the (𝜇_batch , σ_batch) determined during training, and directly fed into equation (3) during evaluation (instead of calling (1) and (2)).\n",
    "\n",
    "**Conclusion**: BN layers make the training faster, and allow a wider range of learning rate without compromising the training convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0b5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
