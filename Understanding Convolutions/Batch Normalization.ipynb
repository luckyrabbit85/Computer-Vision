{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3636d974",
   "metadata": {},
   "source": [
    "# Batch-Normalization\n",
    "\n",
    "Batch-Normalization (BN) is an algorithmic method which makes the training of Deep Neural Networks (DNN) faster and more stable.\n",
    "It consists of normalizing activation vectors from hidden layers using the first and the second statistical moments (mean and variance) of the current batch. This normalization step is applied right before (or right after) the nonlinear function.\n",
    "\n",
    "![](./fig/bn1.jpeg)\n",
    "\n",
    "![](./fig/bn2.jpeg)\n",
    "\n",
    "Batch normalization is computed differently during the training and the testing phase. At each hidden layer, Batch Normalization transforms the signal as follow :\n",
    "\n",
    "![](./fig/bn3.gif)\n",
    "![](./fig/bn4.gif)\n",
    "\n",
    "The BN layer first determines the mean ğœ‡ and the standard deviation Ïƒ of the activation values across the batch, using (1) and (2).\n",
    "It then normalizes the activation vector Z^(i) with (3). That way, each neuronâ€™s output follows a standard normal distribution across the batch. (ğœ€ is a constant used for numerical stability)\n",
    "\n",
    "![](./fig/bn3.jpeg)\n",
    "\n",
    "It finally calculates the layerâ€™s output áº(i) by applying a linear transformation with ğ›¾ and ğ›½, two trainable parameters (4). Such step allows the model to choose the optimum distribution for each hidden layers, by adjusting those two parameters :\n",
    "\n",
    "+ ğ›¾ allows to adjust the standard deviation ;\n",
    "+ ğ›½ allows to adjust the bias, shifting the curve on the right or on the left side.\n",
    "\n",
    "At each iteration, the network computes the mean ğœ‡ and the standard deviation Ïƒ corresponding to the current batch. Then it trains ğ›¾ and ğ›½ through gradient descent, using an Exponential Moving Average (EMA) to give more importance to the latest iterations.\n",
    "\n",
    "![](./fig/3.jpeg)\n",
    "\n",
    "Unlike the training phase, we may not have a full batch to feed into the model during the evaluation phase. To tackle this issue, we compute (ğœ‡_pop , Ïƒ_pop), such as :  \n",
    "\n",
    "+ ğœ‡_pop : estimated mean of the studied population ;\n",
    "+ Ïƒ_pop : estimated standard-deviation of the studied population.  \n",
    "\n",
    "Those values are computed using all the (ğœ‡_batch , Ïƒ_batch) determined during training, and directly fed into equation (3) during evaluation (instead of calling (1) and (2)).\n",
    "\n",
    "**Conclusion**: BN layers make the training faster, and allow a wider range of learning rate without compromising the training convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0b5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
