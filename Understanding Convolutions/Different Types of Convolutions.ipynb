{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3188437",
   "metadata": {},
   "source": [
    "# Convolution in Deep Learning\n",
    "\n",
    "The purpose of doing convolution is to extract useful features from the input. In image processing, there is a wide range of different filters one could choose for convolution. Each type of filters helps to extract different aspects or features from the input image, e.g. horizontal / vertical / diagonal edges. Similarly, in Convolutional Neural Network, different features are extracted through convolution using filters whose weights are automatically learned during training. All these extracted features then are ‘combined’ to make decisions.  Convolution also takes spatial relationship of pixels into considerations. These could be very helpful especially in many computer vision tasks, since those tasks often involve identifying objects where certain components have certain spatially relationship with other components.\n",
    "\n",
    "\n",
    "### Convolution in Deep Learning (single channel version, multi-channel version)\n",
    "\n",
    "The filter is sliding through the input. At each position, it’s doing element-wise multiplication and addition. Each sliding position ends up with one number. The final output is then a 3 x 3 matrix. (Notice that stride = 1 and padding = 0 in this example.\n",
    "\n",
    "![](./fig/CNN.gif)\n",
    "\n",
    "### Convolution: the multi-channel version\n",
    "\n",
    "In this case, first each of the kernels in the filter are applied to three channels (R,G,B) in the input layer, separately. Three convolutions are performed, which result in 3 channels with size 3 x 3\n",
    "\n",
    "![](./fig/1.gif)\n",
    "\n",
    "Then these three channels are summed together (element-wise addition) to form one single channel (3 x 3 x 1). This channel is the result of convolution of the input layer (5 x 5 x 3 matrix) using a filter (3 x 3 x 3 matrix).\n",
    "\n",
    "![](./fig/2.gif)\n",
    "\n",
    "Notice that the input layer and the filter have the same depth (channel number = kernel number). The 3D filter moves only in 2-direction, height & width of the image (That’s why such operation is called as 2D convolution although a 3D filter is used to process 3D volumetric data). At each sliding position, we perform element-wise multiplication and addition, which results in a single number.\n",
    "\n",
    "![](./fig/1.png)\n",
    "\n",
    "Another way to think about 2D convolution: thinking of the process as sliding a 3D filter matrix through the input layer. Notice that the input layer and the filter have the same depth (channel number = kernel number). The 3D filter moves only in 2-direction, height & width of the image (That's why such operation is called as 2D convolution although a 3D filter is used to process 3D volumetric data). The output is a one-layer matrix.\n",
    "\n",
    "![](./fig/2.png)\n",
    "\n",
    "We can see how one can make transitions between layers with different depth. Let’s say the input layer has Din channels, and we want the output layer has Dout channels. What we need to do is to just apply Dout filters to the input layer. Each filter has Din kernels. Each filter provides one output channel. After applying Dout filters, we have Dout channels, which can then be stacked together to form the output layer.\n",
    "\n",
    "### 3D Convolution\n",
    "\n",
    "![](./fig/3.png)\n",
    "\n",
    "In the last illustration of the previous section, we see that we were actually perform convolution to a 3D volume. But typically, we still call that operation as 2D convolution in Deep Learning. *It's a 2D convolution on a 3D volumetric data. The filter depth is same as the input layer depth. The 3D filter moves only in 2-direction (height & width of the image). The output of such operation is a 2D image (with 1 channel only).*\n",
    "\n",
    "Naturally, there are 3D convolutions. They are the generalization of the 2D convolution. *Here in 3D convolution, the filter depth is smaller than the input layer depth (kernel size < channel size). As a result, the 3D filter can move in all 3-direction (height, width, channel of the image).* At each position, the element-wise multiplication and addition provide one number. Since the filter slides through a 3D space, the output numbers are arranged in a 3D space as well. The output is then a 3D data.\n",
    "\n",
    "Use cases: Medical imaging data esp. CT and MRI where objects such as blood vessels meander around in the 3D space.\n",
    "\n",
    "### 1 x 1 Convolution\n",
    "\n",
    "You may wonder why this is helpful. Do we just multiply a number to every number in the input layer? Yes and No. The operation is trivial for layers with only one channel. There, we multiply every element by a number.\n",
    "\n",
    "Things become interesting if the input layer has multiple channels. The following picture illustrates how 1 x 1 convolution works for an input layer with dimension H x W x D. After 1 x 1 convolution with filter size 1 x 1 x D, the output channel is with dimension H x W x 1. If we apply N such 1 x 1 convolutions and then concatenate results together, we could have a output layer with dimension H x W x N.\n",
    "\n",
    "\n",
    "![](./fig/4.png)\n",
    "\n",
    "Initially, 1 x 1 convolutions were proposed in the Network-in-network paper. They were then highly used in the Google Inception paper. A few advantages of 1 x 1 convolutions are:\n",
    "+ Dimensionality reduction for efficient computations\n",
    "+ Efficient low dimensional embedding, or feature pooling\n",
    "+ Applying nonlinearity again after convolution\n",
    "\n",
    "###  Convolution Arithmetic\n",
    "\n",
    "Three main concepts involved in Convolutions are\n",
    "\n",
    "+ **Kernel size**: This defines the field of view of the convolution.\n",
    "\n",
    "+ **Stride**: It defines the step size of the kernel when sliding through the image. Stride of 1 means that the kernel slides through the image pixel by pixel. Stride of 2 means that the kernel slides through image by moving 2 pixels per step (i.e., skipping 1 pixel). We can use stride (>= 2) for downsampling an image.\n",
    "\n",
    "+ **Padding**: the padding defines how the border of an image is handled. A padded convolution ('same' padding in Tensorflow) will keep the spatial output dimensions equal to the input image, by padding 0 around the input boundaries if necessary. On the other hand, unpadded convolution ('valid' padding in Tensorflow) only perform convolution on the pixels of the input image, without adding 0 around the input boundaries. The output size is smaller than the input size.\n",
    "\n",
    "![](./fig/3.gif)\n",
    "\n",
    "The above illustration describes a 2D convolution using a kernel size(K) of 3, stride(S) of 1 and padding(P) of 1. The size of resulting output is same as that of input.\n",
    "\n",
    "The formula to calculate output of convolution:\n",
    "\n",
    "$$Out = \\frac{(W - K + 2P)}{S} + 1$$\n",
    "\n",
    "where, \n",
    "**W** is the input volume \n",
    "**K** is the Kernel size\n",
    "**P** is the padding\n",
    "**S** is the stride\n",
    "\n",
    "###  Transposed Convolution (Deconvolution)\n",
    "\n",
    "For application where we would like to perform upsampling like generating high resolution images, or using auto encoders or semantic segmentation, then we use transposed convolutions.  \n",
    "\n",
    "The transposed convolution is also known as deconvolution, or fractionally strided convolution in the literature. However, it's worth noting that the name \"deconvolution\" is less appropriate, since transposed convolution is not the real deconvolution as defined in signal / image processing. Technically speaking, deconvolution in signal processing reverses the convolution operation. That is not the case here. So it is appropriate to call it Transposed Convolution.\n",
    "\n",
    "It is always possible to implement a transposed convolution with a direct convolution. For an example in the image below, we apply transposed convolution with a 3 x 3 kernel over a 2 x 2 input padded with a 2 x 2 border of zeros using unit strides. The up-sampled output is with size 4 x 4.\n",
    "\n",
    "![](./fig/4.gif)\n",
    "\n",
    "Interestingly enough, one can map the same 2 x 2 input image to a different image size, by applying fancy padding & stride. Below, transposed convolution is applied over the same 2 x 2 input (with 1 zero inserted between inputs) padded with a 2 x 2 border of zeros using unit strides. Now the output is with size 5 x 5.\n",
    "\n",
    "![](./fig/5.gif)\n",
    "\n",
    "In convolution, let us define C as our kernel, Large as the input image, Small as the output image from convolution. After the convolution (matrix multiplication), we down-sample the large image into a small output image. The implementation of convolution in matrix multiplication follows as C x Large = Small.\n",
    "\n",
    "The following example shows how such operation works. It flattens the input to a 16 x 1 matrix, and transforms the kernel into a sparse matrix (4 x 16). The matrix multiplication is then applied between sparse matrix and the flattened input. After that, the resulting matrix (4 x 1) is then transformed back to a 2 x 2 output.\n",
    "\n",
    "![](./fig/1.jpeg)\n",
    "\n",
    "Now, if we multiple the transpose of matrix CT on both sides of the equation, and use the property that multiplication of a matrix with its transposed matrix gives an Unit matrix, then we have the following formula CT x Small = Large, as demonstrated in the figure below.\n",
    "\n",
    "![](./fig/5.png)\n",
    "\n",
    "As you can see here, we perform up-sampling from a small image to a large image. That is what we want to achieve. And now, you can also see where the name “transposed convolution” comes from.\n",
    "\n",
    "### Checkerboard artifacts\n",
    "\n",
    "One unpleasant behavior that people observe when using transposed convolution is the so-called checkerboard artifacts.\n",
    "\n",
    "![](./fig/ch.png)\n",
    "\n",
    "Checkerboard artifacts result from “uneven overlap” of transposed convolution. Such overlap puts more of the metaphorical paint in some places than others. In the image below, the layer on the top is the input layer, and the layer on the bottom is the output layer after transposed convolution. During transposed convolution, a layer with small size is mapped to a layer with larger size.\n",
    "\n",
    "In the example (a), the stride is 1 and the filer size is 2. As outlined in red, the first pixel on the input maps to the first and second pixels on the output. As outlined in green, the second pixel on the input maps to the second and the third pixels on the output. The second pixel on the output receives information from both the first and the second pixels on the input. Overall, the pixels in the middle portion of the output receive same amount of information from the input. Here exist a region where kernels overlapped. As the filter size is increased to 3 in the example (b), the center portion that receives most information shrinks. But this may not be a big deal, since the overlap is still even. The pixels in the center portion of the output receive same amount of information from the input.\n",
    "\n",
    "![](./fig/ch1.png)\n",
    "\n",
    "Now for the example below, we change stride = 2. In the example (a) where filter size = 2, all pixels on the output receive same amount of information from the input. They all receive information from a single pixel on the input. There is no overlap of transposed convolution here.\n",
    "\n",
    "![](./fig/ch2.png)\n",
    "\n",
    "If we change the filter size to 4 in the example (b), the evenly overlapped region shrinks. But still, one can use the center portion of the output as the valid output, where each pixel receives the same amount of information from the input.\n",
    "However, things become interesting if we change the filter size to 3 and 5 in the example (c) and (d). For these two cases, every pixel on the output receives different amount of information compared to its adjacent pixels. One cannot find a continuous and evenly overlapped region on the output.\n",
    "\n",
    "The transposed convolution has uneven overlap when the filter size is not divisible by the stride. This “uneven overlap” puts more of the paint in some places than others, thus creates the checkerboard effects. In fact, the unevenly overlapped region tends to be more extreme in two dimensions. There, two patterns are multiplied together, the unevenness gets squared.\n",
    "Two things one could do to reduce such artifacts, while applying transposed convolution. First, make sure you use a filer size that is divided by your stride, avoiding the overlap issue. Secondly, one can use transposed convolution with stride = 1, which helps to reduce the checkerboard effects. However, artifacts can still leak through, as seen in many recent models.\n",
    "\n",
    "\n",
    "### Dilated convolutions\n",
    "\n",
    "Intuitively, dilated convolutions “inflate” the kernel by inserting spaces between the kernel elements. This additional parameter l (dilation rate) indicates how much we want to widen the kernel. Implementations may vary, but there are usually l-1 spaces inserted between kernel elements. The following image shows the kernel size when l = 1, 2, and 4.\n",
    "\n",
    "![](./fig/6.gif)\n",
    "\n",
    "Receptive field for the dilated convolution. We essentially observe a large receptive field without adding additional costs.\n",
    "\n",
    "![](./fig/2.jpeg)\n",
    "\n",
    "In the image, the 3 x 3 red dots indicate that after the convolution, the output image is with 3 x 3 pixels. Although all three dilated convolutions provide the output with the same dimension, the receptive field observed by the model is dramatically different. The receptive filed is 3 x 3 for l =1. It is 7 x 7 for l =2. The receptive filed increases to 15 x 15 for l = 4. \n",
    "\n",
    "Interestingly, the numbers of parameters associated with these operations are essentially identical. We “observe” a large receptive filed without adding additional costs. Because of that, dilated convolution is used to cheaply increase the receptive field of output units without increasing the kernel size, which is especially effective when multiple dilated convolutions are stacked one after another.\n",
    "\n",
    "### Separable Convolutions\n",
    "\n",
    "Spatially separable convolution decomposes a convolution into two separate operations such that one need less matrix multiplications in spatially separable convolution than convolution. Although spatially separable convolutions save cost, it is rarely used in deep learning. One of the main reason is that not all kernels can be divided into two, smaller kernels. If we replace all traditional convolutions by the spatially separable convolution, we limit ourselves for searching all possible kernels during training. The training results may be sub-optimal.\n",
    "\n",
    "Lets look on to the depthwise separable convolutions, which is much more commonly used in deep learning (e.g. in MobileNet and Xception). The depth wise separable convolutions consist of two steps: depthwise convolutions and 1x1 convolutions. The following diagram will make it clear.\n",
    "\n",
    "\n",
    "![](./fig/dsc.png)\n",
    "\n",
    "\n",
    "So we see T2 << T1 and this is how performing depth wise convolutions reduces costs dramaticaly and favored in small models which are deployed on edge devices. THe main advantage here is Efficiency! One needs much less operations for depthwise separable convolutions compared to 2D convolutions.\n",
    "\n",
    "The depthwise separable convolutions reduces the number of parameters in the convolution. As such, for a small model, the model capacity may be decreased significantly if the 2D convolutions are replaced by depthwise separable convolutions. As a result, the model may become sub-optimal. However, if properly used, depthwise separable convolutions can give you the efficiency without dramatically damaging your model performance.\n",
    "\n",
    "### Grouped Convolution\n",
    "\n",
    "In general we have input layer of size (Hin x Win x Din) is transformed into the output layer of size (Hout x Wout x Dout) by applying Dout kernels (each is of size h x w x Din).\n",
    "\n",
    "![](./fig/g1.png)\n",
    "\n",
    "In grouped convolution, the filters are separated into different groups. Each group is responsible for a conventional 2D convolutions with certain depth. The following examples can make this clearer.\n",
    "\n",
    "![](./fig/g2.png)\n",
    "\n",
    "Above is the illustration of grouped convolution with 2 filter groups. In each filter group, the depth of each filter is only half of the that in the nominal 2D convolutions. They are of depth Din / 2. Each filter group contains Dout /2 filters. The first filter group (red) convolves with the first half of the input layer ([:, :, 0:Din/2]), while the second filter group (blue) convolves with the second half of the input layer ([:, :, Din/2:Din]). As a result, each filter group creates Dout/2 channels. Overall, two groups create 2 x Dout/2 = Dout channels. We then stack these channels in the output layer with Dout channels\n",
    "\n",
    "The **first advantage** is the efficient training. Since the convolutions are divided into several paths, each path can be handled separately by different GPUs. This procedure allows the model training over multiple GPUs, in a parallel fashion. Such model-parallelization over multi-GPUs allows more images to be fed into the network per step, compared to training with everything with one GPU. The model-parallelization is considered to be better than data parallelization. The later one split the dataset into batches and then we train on each batch. However, when the batch size becomes too small, we are essentially doing stochastic than batch gradient descent. This would result in slower and sometimes poorer convergence.\n",
    "\n",
    "The grouped convolutions become important for training very deep neural nets, as in the ResNeXt shown below\n",
    "\n",
    "![](./fig/g3.png)\n",
    "\n",
    "The **second advantage** is the model is more efficient, i.e. the model parameters decrease as number of filter group increases. In the previous examples, filters have h x w x Din x Dout parameters in a nominal 2D convolution. Filters in a grouped convolution with 2 filter groups has (h x w x Din/2 x Dout/2) x 2 parameters. The number of parameters is reduced by half.\n",
    "\n",
    "The **third advantage** is a bit surprising. Grouped convolution may provide a better model than a nominal 2D convolution.\n",
    "\n",
    "### Shuffled Grouped Convolution\n",
    "\n",
    "Shuffled grouped convolution was introduced in the ShuffleNet from Magvii Inc (Face++). ShuffleNet is a computation-efficient convolution architecture, which is designed specially for mobile devices with very limited computing power (e.g. 10–150 MFLOPs).\n",
    "\n",
    "The ideas behind the shuffled grouped convolution are linked to the ideas behind grouped convolution (used in MobileNet and ResNeXt for examples) and depthwise separable convolution (used in Xception). Overall, the shuffled grouped convolution involves grouped convolution and channel shuffling.\n",
    "\n",
    "In the section about grouped convolution, we know that the filters are separated into different groups. Each group is responsible for a conventional 2D convolutions with certain depth. The total operations are significantly reduced. For examples in the figure below, we have 3 filter groups. The first filter group convolves with the red portion in the input layer. Similarly, the second and the third filter group convolves with the green and blue portions in the input. The kernel depth in each filter group is only 1/3 of the total channel count in the input layer. In this example, after the first grouped convolution GConv1, the input layer is mapped to the intermediate feature map. This feature map is then mapped to the output layer through the second grouped convolution GConv2.\n",
    "\n",
    "![](./fig/s1.png)\n",
    "\n",
    "Grouped convolution is computationally efficient. But the problem is that each filter group only handles information passed down from the fixed portion in the previous layers. For examples in the image above, the first filter group (red) only process information that is passed down from the first 1/3 of the input channels. The blue filter group (blue) only process information that is passed down from the last 1/3 of the input channels. As such, each filter group is only limited to learn a few specific features. This property blocks information flow between channel groups and weakens representations during training. To overcome this problem, we apply the channel shuffle.\n",
    "\n",
    "The idea of channel shuffle is that we want to mix up the information from different filter groups. In the image below, we get the feature map after applying the first grouped convolution GConv1 with 3 filter groups. Before feeding this feature map into the second grouped convolution, we first divide the channels in each group into several subgroups. The we mix up these subgroups.\n",
    "\n",
    "![](./fig/s2.png)\n",
    "\n",
    "After such shuffling, we continue performing the second grouped convolution GConv2 as usual. But now, since the information in the shuffled layer has already been mixed, we essentially feed each group in GConv2 with different subgroups in the feature map layer (or in the input layer). As a result, we allow the information flow between channels groups and strengthen the representations.\n",
    "\n",
    "###  Pointwise grouped convolution\n",
    "\n",
    "The ShuffleNet paper also introduced the pointwise grouped convolution. Typically for grouped convolution such as in MobileNet or ResNeXt, the group operation is performed on the 3x3 spatial convolution, but not on 1 x 1 convolution.\n",
    "\n",
    "The shuffleNet paper argues that the 1 x 1 convolution are also computationally costly. It suggests applying group convolution for 1 x 1 convolution as well. The pointwise grouped convolution, as the name suggested, performs group operations for 1 x 1 convolution. The operation is identical as for grouped convolution, with only one modification - performing on 1x1 filters instead of NxN filters (N>1).\n",
    "\n",
    "In the ShuffleNet paper, authors utilized three types of convolutions we have learned: (1) shuffled grouped convolution; (2) pointwise grouped convolution; and (3) depthwise separable convolution. Such architecture design significantly reduces the computation cost while maintaining the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef6247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
