{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3188437",
   "metadata": {},
   "source": [
    "# Convolution in Deep Learning\n",
    "\n",
    "The purpose of doing convolution is to extract useful features from the input. In image processing, there is a wide range of different filters one could choose for convolution. Each type of filters helps to extract different aspects or features from the input image, e.g. horizontal / vertical / diagonal edges. Similarly, in Convolutional Neural Network, different features are extracted through convolution using filters whose weights are automatically learned during training. All these extracted features then are ‘combined’ to make decisions.  Convolution also takes spatial relationship of pixels into considerations. These could be very helpful especially in many computer vision tasks, since those tasks often involve identifying objects where certain components have certain spatially relationship with other components.\n",
    "\n",
    "\n",
    "### Convolution in Deep Learning (single channel version, multi-channel version)\n",
    "\n",
    "The filter is sliding through the input. At each position, it’s doing element-wise multiplication and addition. Each sliding position ends up with one number. The final output is then a 3 x 3 matrix. (Notice that stride = 1 and padding = 0 in this example.\n",
    "\n",
    "![](./fig/CNN.gif)\n",
    "\n",
    "### Convolution: the multi-channel version\n",
    "\n",
    "In this case, first each of the kernels in the filter are applied to three channels (R,G,B) in the input layer, separately. Three convolutions are performed, which result in 3 channels with size 3 x 3\n",
    "\n",
    "![](./fig/1.gif)\n",
    "\n",
    "Then these three channels are summed together (element-wise addition) to form one single channel (3 x 3 x 1). This channel is the result of convolution of the input layer (5 x 5 x 3 matrix) using a filter (3 x 3 x 3 matrix).\n",
    "\n",
    "![](./fig/2.gif)\n",
    "\n",
    "Notice that the input layer and the filter have the same depth (channel number = kernel number). The 3D filter moves only in 2-direction, height & width of the image (That’s why such operation is called as 2D convolution although a 3D filter is used to process 3D volumetric data). At each sliding position, we perform element-wise multiplication and addition, which results in a single number.\n",
    "\n",
    "![](./fig/1.png)\n",
    "\n",
    "Another way to think about 2D convolution: thinking of the process as sliding a 3D filter matrix through the input layer. Notice that the input layer and the filter have the same depth (channel number = kernel number). The 3D filter moves only in 2-direction, height & width of the image (That's why such operation is called as 2D convolution although a 3D filter is used to process 3D volumetric data). The output is a one-layer matrix.\n",
    "\n",
    "![](./fig/2.png)\n",
    "\n",
    "We can see how one can make transitions between layers with different depth. Let’s say the input layer has Din channels, and we want the output layer has Dout channels. What we need to do is to just apply Dout filters to the input layer. Each filter has Din kernels. Each filter provides one output channel. After applying Dout filters, we have Dout channels, which can then be stacked together to form the output layer.\n",
    "\n",
    "### 3D Convolution\n",
    "\n",
    "![](./fig/3.png)\n",
    "\n",
    "In the last illustration of the previous section, we see that we were actually perform convolution to a 3D volume. But typically, we still call that operation as 2D convolution in Deep Learning. *It's a 2D convolution on a 3D volumetric data. The filter depth is same as the input layer depth. The 3D filter moves only in 2-direction (height & width of the image). The output of such operation is a 2D image (with 1 channel only).*\n",
    "\n",
    "Naturally, there are 3D convolutions. They are the generalization of the 2D convolution. *Here in 3D convolution, the filter depth is smaller than the input layer depth (kernel size < channel size). As a result, the 3D filter can move in all 3-direction (height, width, channel of the image).* At each position, the element-wise multiplication and addition provide one number. Since the filter slides through a 3D space, the output numbers are arranged in a 3D space as well. The output is then a 3D data.\n",
    "\n",
    "Use cases: Medical imaging data esp. CT and MRI where objects such as blood vessels meander around in the 3D space.\n",
    "\n",
    "### 1 x 1 Convolution\n",
    "\n",
    "You may wonder why this is helpful. Do we just multiply a number to every number in the input layer? Yes and No. The operation is trivial for layers with only one channel. There, we multiply every element by a number.\n",
    "\n",
    "Things become interesting if the input layer has multiple channels. The following picture illustrates how 1 x 1 convolution works for an input layer with dimension H x W x D. After 1 x 1 convolution with filter size 1 x 1 x D, the output channel is with dimension H x W x 1. If we apply N such 1 x 1 convolutions and then concatenate results together, we could have a output layer with dimension H x W x N.\n",
    "\n",
    "\n",
    "![](./fig/4.png)\n",
    "\n",
    "Initially, 1 x 1 convolutions were proposed in the Network-in-network paper. They were then highly used in the Google Inception paper. A few advantages of 1 x 1 convolutions are:\n",
    "+ Dimensionality reduction for efficient computations\n",
    "+ Efficient low dimensional embedding, or feature pooling\n",
    "+ Applying nonlinearity again after convolution\n",
    "\n",
    "###  Convolution Arithmetic\n",
    "\n",
    "Three main concepts involved in Convolutions are\n",
    "\n",
    "+ **Kernel size**: This defines the field of view of the convolution.\n",
    "\n",
    "+ **Stride**: It defines the step size of the kernel when sliding through the image. Stride of 1 means that the kernel slides through the image pixel by pixel. Stride of 2 means that the kernel slides through image by moving 2 pixels per step (i.e., skipping 1 pixel). We can use stride (>= 2) for downsampling an image.\n",
    "\n",
    "+ **Padding**: the padding defines how the border of an image is handled. A padded convolution ('same' padding in Tensorflow) will keep the spatial output dimensions equal to the input image, by padding 0 around the input boundaries if necessary. On the other hand, unpadded convolution ('valid' padding in Tensorflow) only perform convolution on the pixels of the input image, without adding 0 around the input boundaries. The output size is smaller than the input size.\n",
    "\n",
    "![](./fig/3.gif)\n",
    "\n",
    "The above illustration describes a 2D convolution using a kernel size(K) of 3, stride(S) of 1 and padding(P) of 1. The size of resulting output is same as that of input.\n",
    "\n",
    "The formula to calculate output of convolution:\n",
    "\n",
    "$$Out = \\frac{(W - K + 2P)}{S} + 1$$\n",
    "\n",
    "where, \n",
    "**W** is the input volume \n",
    "**K** is the Kernel size\n",
    "**P** is the padding\n",
    "**S** is the stride\n",
    "\n",
    "###  Transposed Convolution (Deconvolution)\n",
    "\n",
    "For application where we would like to perform upsampling like generating high resolution images, or using auto encoders or semantic segmentation, then we use transposed convolutions.  \n",
    "\n",
    "The transposed convolution is also known as deconvolution, or fractionally strided convolution in the literature. However, it's worth noting that the name \"deconvolution\" is less appropriate, since transposed convolution is not the real deconvolution as defined in signal / image processing. Technically speaking, deconvolution in signal processing reverses the convolution operation. That is not the case here. So it is appropriate to call it Transposed Convolution.\n",
    "\n",
    "It is always possible to implement a transposed convolution with a direct convolution. For an example in the image below, we apply transposed convolution with a 3 x 3 kernel over a 2 x 2 input padded with a 2 x 2 border of zeros using unit strides. The up-sampled output is with size 4 x 4.\n",
    "\n",
    "![](./fig/4.gif)\n",
    "\n",
    "Interestingly enough, one can map the same 2 x 2 input image to a different image size, by applying fancy padding & stride. Below, transposed convolution is applied over the same 2 x 2 input (with 1 zero inserted between inputs) padded with a 2 x 2 border of zeros using unit strides. Now the output is with size 5 x 5.\n",
    "\n",
    "![](./fig/5.gif)\n",
    "\n",
    "In convolution, let us define C as our kernel, Large as the input image, Small as the output image from convolution. After the convolution (matrix multiplication), we down-sample the large image into a small output image. The implementation of convolution in matrix multiplication follows as C x Large = Small.\n",
    "\n",
    "The following example shows how such operation works. It flattens the input to a 16 x 1 matrix, and transforms the kernel into a sparse matrix (4 x 16). The matrix multiplication is then applied between sparse matrix and the flattened input. After that, the resulting matrix (4 x 1) is then transformed back to a 2 x 2 output.\n",
    "\n",
    "![](./fig/1.jpeg)\n",
    "\n",
    "Now, if we multiple the transpose of matrix CT on both sides of the equation, and use the property that multiplication of a matrix with its transposed matrix gives an Unit matrix, then we have the following formula CT x Small = Large, as demonstrated in the figure below.\n",
    "\n",
    "![](./fig/5.png)\n",
    "\n",
    "As you can see here, we perform up-sampling from a small image to a large image. That is what we want to achieve. And now, you can also see where the name “transposed convolution” comes from.\n",
    "\n",
    "### Checkerboard artifacts\n",
    "\n",
    "One unpleasant behavior that people observe when using transposed convolution is the so-called checkerboard artifacts.\n",
    "\n",
    "![](./fig/ch.png)\n",
    "\n",
    "Checkerboard artifacts result from “uneven overlap” of transposed convolution. Such overlap puts more of the metaphorical paint in some places than others. In the image below, the layer on the top is the input layer, and the layer on the bottom is the output layer after transposed convolution. During transposed convolution, a layer with small size is mapped to a layer with larger size.\n",
    "\n",
    "In the example (a), the stride is 1 and the filer size is 2. As outlined in red, the first pixel on the input maps to the first and second pixels on the output. As outlined in green, the second pixel on the input maps to the second and the third pixels on the output. The second pixel on the output receives information from both the first and the second pixels on the input. Overall, the pixels in the middle portion of the output receive same amount of information from the input. Here exist a region where kernels overlapped. As the filter size is increased to 3 in the example (b), the center portion that receives most information shrinks. But this may not be a big deal, since the overlap is still even. The pixels in the center portion of the output receive same amount of information from the input.\n",
    "\n",
    "![](./fig/ch1.png)\n",
    "\n",
    "Now for the example below, we change stride = 2. In the example (a) where filter size = 2, all pixels on the output receive same amount of information from the input. They all receive information from a single pixel on the input. There is no overlap of transposed convolution here.\n",
    "\n",
    "![](./fig/ch2.png)\n",
    "\n",
    "If we change the filter size to 4 in the example (b), the evenly overlapped region shrinks. But still, one can use the center portion of the output as the valid output, where each pixel receives the same amount of information from the input.\n",
    "However, things become interesting if we change the filter size to 3 and 5 in the example (c) and (d). For these two cases, every pixel on the output receives different amount of information compared to its adjacent pixels. One cannot find a continuous and evenly overlapped region on the output.\n",
    "\n",
    "The transposed convolution has uneven overlap when the filter size is not divisible by the stride. This “uneven overlap” puts more of the paint in some places than others, thus creates the checkerboard effects. In fact, the unevenly overlapped region tends to be more extreme in two dimensions. There, two patterns are multiplied together, the unevenness gets squared.\n",
    "Two things one could do to reduce such artifacts, while applying transposed convolution. First, make sure you use a filer size that is divided by your stride, avoiding the overlap issue. Secondly, one can use transposed convolution with stride = 1, which helps to reduce the checkerboard effects. However, artifacts can still leak through, as seen in many recent models.\n",
    "\n",
    "\n",
    "### Dilated convolutions\n",
    "\n",
    "Intuitively, dilated convolutions “inflate” the kernel by inserting spaces between the kernel elements. This additional parameter l (dilation rate) indicates how much we want to widen the kernel. Implementations may vary, but there are usually l-1 spaces inserted between kernel elements. The following image shows the kernel size when l = 1, 2, and 4.\n",
    "\n",
    "![](./fig/6.gif)\n",
    "\n",
    "Receptive field for the dilated convolution. We essentially observe a large receptive field without adding additional costs.\n",
    "\n",
    "![](./fig/2.jpeg)\n",
    "\n",
    "In the image, the 3 x 3 red dots indicate that after the convolution, the output image is with 3 x 3 pixels. Although all three dilated convolutions provide the output with the same dimension, the receptive field observed by the model is dramatically different. The receptive filed is 3 x 3 for l =1. It is 7 x 7 for l =2. The receptive filed increases to 15 x 15 for l = 4. \n",
    "\n",
    "Interestingly, the numbers of parameters associated with these operations are essentially identical. We “observe” a large receptive filed without adding additional costs. Because of that, dilated convolution is used to cheaply increase the receptive field of output units without increasing the kernel size, which is especially effective when multiple dilated convolutions are stacked one after another.\n",
    "\n",
    "### Separable Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef6247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
